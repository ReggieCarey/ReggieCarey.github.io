<!doctype html>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">

<script type="text/javascript" src="https://distill.pub/template.v1.js"></script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" src="https://d3js.org/d3.v5.min.js"></script>

<style>
    dt-code code {
        border: 1px solid #d0d0d0;
        border-radius: 10px;
        background-color: #f0f0f0;
    }

    code[class*="language-"], pre[class*="language-"] {
        text-shadow: 0.5px 0.5px 5px rgba(0, 0, 0, 0.05);
    }

    .token.operator, .token.entity, .token.url, .language-css .token.string, .style .token.string {
        /*background-color: #777777;*/
    }

    html, dt-article, pre code {
        /*background-color: #777777;*/
    }
</style>

<script type="text/front-matter">
    title: "Artifical General Intelligence via Biologically Inspired Neural Constructs"
    description: "Artificial General Intelligence (AGI) is a ..."
    authors:
    - Reginald Carey: http://carey-agi.com
    affiliations:
    - UMD NeuroTheory Lab: http://neurotheory.umd.edu
</script>

<dt-article>

    <div class="l-page">
        <h1>Artifical General Intelligence via Biologically Inspired Neural Constructs</h1>
        <p>Artificial General Intelligence<dt-fn>AGI - artificial general intelligence</dt-fn> is a ...
        As we look at the world around us, we have immediate access to the composition of the visual scene into objects, as well
        as our relationship in space to those objects. Likewise, in listening to speech, we are aware of meaning, often without
        even paying attention to the words themselves.
        </p>

        <dt-byline></dt-byline>
        <p>As we look at the world around us, we have immediate access to the composition of the visual scene into objects, as well
            as our relationship in space to those objects. Likewise, in listening to speech, we are aware of meaning, often without
            even paying attention to the words themselves. This natural facility makes it possible to move though the world, catch
            or avoid moving objects, and base immediate decisions on a detailed understanding of the world around us. Only
            secondarily might we note the particular color or composition of particular points in the visual scene, or the durations
            of certain vowel sounds, or other low-level visual or auditory features of the scene. Our brain processes sensory
            information much differently than computers do: a computer can easily store the hue and luminance of every pixel of an
            image, but even with the best available software it cannot parse an arbitrary natural image into its underlying
            elements.</p>
        <p>Paragraph 2 with a citation
            <dt-cite key="carey2019"></dt-cite>
            about my research.
        </p>

        <p>Equations</p>
        <p>
            $$
            Attention(Q,K,V) := Softmax(\frac{QK^T}{\sqrt{d_k}})V
            $$
        </p>
        <dt-code block language="python">
class SelfAttention(tf.keras.layers.Layer):
    """
    With SelfAttention class we assume the input to be transformed into query, key, value vectors
    and stored in "input" (to call()) as a tuple.

    This implementation can operate in the multihead regime.  The trick is to change the dimensionality of
    query, key, and value by prefixing a dimension for the number of selfAttention heads.  The result is a tensor
    of the same shape as the input
    """

    def __init__(self, masked: bool = False, **kwargs) -> None:
        super(SelfAttention, self).__init__(**kwargs)
        self.masked = masked
        self.softmax = tf.keras.layers.Softmax()

    def build(self, input_shape):
        super().build(input_shape)
        (q_shape, k_shape, v_shape) = input_shape
        # Our key width comes from the query
        self.d_k = q_shape[-2].value
        # Calculate what the transpose order looks like for the tensors to be transposed
        to = [x for x in range(len(k_shape))]
        to[-2], to[-1] = to[-1], to[-2]
        # Setup a transpose function that uses our transpose order.
        self.T = partial(tf.transpose, perm=to)
        # Calculate the scaling factor used to make softmax more stable
        self.scale = self.d_k ** -.5
        # Calculate the shape and size of a mask if its enabled
        size = (q_shape[-2], k_shape[-2])
        if self.masked:
            self.mask = np.tril(np.ones(size))
        else:
            self.mask = np.ones(size)

    def call(self, inputs, **kwargs):
        (Q, K, V) = inputs
        with tf.compat.v1.variable_scope("scaledDotProductAttention"):
            result = Q @ self.T(K)  # Dot Product - which rows in K resonate with Q
            result = self.scale * result  # Scale the dot product - this is a numeric stability measure
            result = (result * self.mask) + ((1. - self.mask) * -1e10)  # Apply a mask to the input (pointwise)
            result = self.softmax(result)  # Compute a distribution for each word in Q
            result = result @ V  # Select and blend word(s) in V that resonate with the query (Q)
            return result
        </dt-code>
    </div>
</dt-article>

<dt-appendix>
    <h3>Acknowledgements</h3>
    <p>section content</p>
    <h3>Author Contributions</h3>
    <p>section content</p>
</dt-appendix>

<script type="text/bibliography">
    @article{carey2019,
        title={GOOBER: A Transformer for Neural Theory Evaluation},
        author={Carey, Reginald},
        journal={arXivreprint arXiv:1234.05132},
        year={2019},
        url={http://arxiv.org/pdf/1234.05132.pdf}
    }
</script>

<!-- Google Analytics: change UA-XXXXX-X to be your site's ID. -->
<script>
    (function (b, o, i, l, e, r) {
        b.GoogleAnalyticsObject = l;
        b[l] || (b[l] =
            function () {
                (b[l].q = b[l].q || []).push(arguments)
            });
        b[l].l = +new Date;
        e = o.createElement(i);
        r = o.getElementsByTagName(i)[0];
        e.src = '//www.google-analytics.com/analytics.js';
        r.parentNode.insertBefore(e, r)
    }(window, document, 'script', 'ga'));
    ga('create', 'UA-XXXXX-X', 'auto');
    ga('send', 'pageview');
</script>
